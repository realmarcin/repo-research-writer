# Introduction

## Background and Motivation

Computational researchers routinely generate substantial amounts of data through code-based analyses, simulations, and data processing pipelines. However, translating these computational artifacts—code scripts, data files, Jupyter notebooks, and figures—into coherent, publication-ready manuscripts remains a largely manual, time-intensive process. This translation stage is prone to transcription errors when researchers manually copy numerical results from data files into text, broken provenance chains when the connection between claims and their supporting evidence becomes obscured, and inconsistent methodological descriptions when code logic is paraphrased rather than systematically documented [kanwal2017provenance]. The problem is particularly acute in bioinformatics and computational biology, where complex analytical workflows must be accurately communicated to enable reproducibility [rule2018jupyter].

Current practices require researchers to maintain mental mappings between their repository structure and manuscript sections, manually verify that reported statistics match source data, and repeatedly reformat content when targeting different journals. This manual workflow not only consumes significant researcher time but also introduces opportunities for error at each transcription step. Despite widespread recognition of reproducibility challenges in computational research, existing tools address only fragments of this workflow—citation management, notebook execution, or workflow documentation—without providing end-to-end automation that maintains rigorous fact-checking throughout the manuscript generation process.

## Existing Approaches and Limitations

Several categories of tools have emerged to address aspects of scientific writing and reproducibility. Literature review platforms such as Rayyan [rayyan], Elicit [elicit], and Semantic Scholar [semanticscholar] facilitate systematic review and citation discovery but do not generate manuscript text. AI-assisted writing tools including Paperpal [paperpal], SciSpace [scispace], and Paperguide [paperguide] offer general writing assistance but lack integration with research code repositories and provide no mechanisms for fact verification against source data. Recent work has demonstrated that large language models like GPT-4 can generate scientific text comparable to human-written content [wang2024gpt4], and evidence suggests increasing adoption of LLM-assisted writing in biomedical publications [kobak2025excess].

For reproducible research workflows, Jupyter notebooks have become widely adopted [rule2018jupyter], and provenance tracking systems like ReproZip [chirigati2013reprozip] and WorkflowHub [workflowhub] capture computational dependencies. However, these systems focus on workflow execution rather than manuscript generation. The AI Scientist project represents a recent advance toward automated scientific discovery [lu2024scientist], but its emphasis is on conducting novel experiments rather than documenting existing computational research results in manuscript form.

A critical gap exists in the integration of these capabilities: no existing system provides end-to-end manuscript generation directly from research code repositories while enforcing mandatory fact-checking of all numerical claims against source data files. This gap is particularly problematic given evidence that automated fact-checking remains challenging in scientific contexts [guo2022survey].

## RRWrite System Contribution

We present RRWrite (Repo Research Writer), an AI-powered system that addresses the complete manuscript generation workflow from computational research repositories. RRWrite implements a five-stage pipeline: (1) repository analysis and journal-specific outline generation, (2) deep literature research with citation management, (3) section-by-section drafting with mandatory fact verification, (4) adversarial critique against journal standards, and (5) final manuscript assembly with schema-based validation. The system is built on the Claude Code framework using a modular skill architecture, with four core skills (plan-manuscript, research-literature, draft-section, critique-manuscript) that coordinate through a shared file system and state tracking mechanism.

Unlike prior work, RRWrite enforces fact-checking at the drafting stage rather than as an optional post-hoc verification step. Before any numerical claim is finalized in manuscript text, the system requires verification against source CSV or Excel files using deterministic Python scripts. This verification-by-design approach maintains complete provenance chains from raw data files to manuscript claims. The system additionally implements multi-journal format support through template-based planning, enabling researchers to rapidly reformat manuscripts for different venues (Nature Methods, PLOS Computational Biology, Bioinformatics) without manual restructuring.

RRWrite is implemented as open-source software with a global installation model enabling reuse across multiple research projects. The system integrates Git-based version control with JSON state tracking for workflow progress, and employs LinkML schemas [matentzoglu2025linkml] for validation of manuscript structure and content. By combining the agentic capabilities of large language models with deterministic verification tools, RRWrite provides researchers with an automated yet scientifically rigorous approach to manuscript generation from computational research artifacts.
