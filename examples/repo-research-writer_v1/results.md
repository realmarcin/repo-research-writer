# Results

## Core Capabilities

RRWrite provides a comprehensive manuscript generation system with capabilities spanning repository analysis, evidence validation, literature integration, and multi-journal formatting. The system implements four core AI skills with a combined total of 1,600+ lines of skill-specific protocol definitions distributed across `.claude/skills/rrwrite-plan-manuscript/`, `.claude/skills/rrwrite-draft-section/`, `.claude/skills/rrwrite-research-literature/`, and `.claude/skills/rrwrite-critique-manuscript/`. These skills are supported by 10 Python verification and state management tools totaling 2,481 lines of implementation code.

The planning skill (`rrwrite-plan-manuscript`) supports three major journal formats: Nature Methods, PLOS Computational Biology, and Bioinformatics. For each journal, the skill applies format-specific constraints including section ordering requirements, word count limits (e.g., 800-1200 words for Bioinformatics Application Notes, 3000-5000 words for PLOS Computational Biology Methods), and structural conventions (e.g., combined Results/Discussion for Nature Methods, separate Algorithm and Implementation sections for Bioinformatics). The skill generates structured outlines in Markdown format stored as `manuscript/outline.md`, linking each planned section to specific evidence files in the repository with explicit line number references where appropriate.

Fact verification operates through the `rrwrite-verify-stats.py` tool, which accepts CSV and Excel files as input and computes descriptive statistics (mean, median, standard deviation, minimum, maximum, count) for specified columns. The tool supports pandas DataFrames for CSV processing and openpyxl for Excel workbook reading, enabling verification across common scientific data formats. During section drafting, numerical claims trigger verification commands such as `python scripts/rrwrite-verify-stats.py --file data/results.csv --col accuracy --op mean`, with the returned value compared against the drafted text before finalization. This mandatory verification step prevents transcription errors and maintains provenance from raw data files to published claims.

Citation management integrates BibTeX bibliography files through a multi-file system. User-provided citations are stored in `references.bib` at the project root, while literature research generates `manuscript/literature_citations.bib` containing newly discovered papers. The research skill (`rrwrite-research-literature`) performs web searches to identify foundational work (pre-2020), recent advances (2024-2026), and competing methods, extracting DOIs and generating standardized BibTeX entries for each source. Additionally, the skill creates `manuscript/literature_evidence.csv` containing direct quotes from each cited work, enabling verification that manuscript statements accurately represent source material. Citation keys follow a standardized format (first author surname + year, e.g., `smith2020`) and are inserted into manuscript text using bracket notation (e.g., `[smith2020]`).

Git integration provides version control for manuscript artifacts while preserving workflow state tracking orthogonally. The `rrwrite-state-manager.py` module (552 lines) implements methods for recording Git commit hashes, branch names, and file modification timestamps alongside workflow progress in `.rrwrite/state.json`. State updates occur atomically after successful skill execution, with file locking preventing concurrent write conflicts. The system tracks five workflow stages (plan, research, draft, critique, assembly) with granular substage tracking such as number of sections completed (e.g., "3/5 sections drafted"), critique iteration counts, and timestamps for each milestone. Git operations are invoked via subprocess calls to standard Git commands, with checkpoint warnings issued before any action that would overwrite uncommitted changes.

## Example Output

The example project in the `example/` directory demonstrates manuscript generation for a protein structure prediction study. The input repository contains `PROJECT.md` documenting key findings (accuracy improvements, computational efficiency gains), `data/benchmark_results.csv` with model performance metrics, `scripts/train_model.py` and `scripts/evaluate.py` implementing the methodology, and `figures/accuracy_comparison.png` visualizing results. From these inputs, the planning skill generates a 280-line outline (`manuscript_plan.md`) mapping Abstract (6 lines) to performance metrics in `benchmark_results.csv`, Introduction (31 lines) to background from `PROJECT.md` and related work citations, Methods (89 lines) to code implementations in training and evaluation scripts, Results (43 lines) to data files and figures, and Discussion (29 lines) to interpretations in `PROJECT.md`.

The generated outline structure follows Bioinformatics journal conventions with 180-word Abstract, 500-word Introduction, 700-word Methods (subdivided into Model Architecture, Training Procedure, Evaluation Protocol), 450-word Results (subdivided into Accuracy Benchmarks, Computational Performance), and 400-word Discussion. Evidence files are explicitly linked: Abstract specifies "PROJECT.md (lines 1-15: project overview)", Methods references "scripts/train_model.py (lines 45-89: optimizer configuration)", and Results points to "data/benchmark_results.csv (column: tm_score)" with instructions to verify reported statistics using `rrwrite-verify-stats.py`.

Section word count targets align with journal requirements while allowing 10-20% flexibility to accommodate natural prose boundaries. The outline includes specific instructions for each subsection, such as "Use passive voice for Methods" and "Report mean ± std for accuracy metrics with verification before finalization." Figure references are pre-planned with captions to be derived from plotting scripts: "Figure 1 visualizes the t-SNE projection of learned embeddings, generated by scripts/visualize.py (lines 23-45)."

Citation formatting demonstrates integration with the BibTeX system. The Introduction outline specifies citations to foundational work on protein folding (`[anfinsen1973]`, `[dill2008]`) and recent deep learning approaches (`[alphafold2021]`, `[rosettafold2021]`), with all entries present in `example/references.bib`. The literature evidence file (`literature_evidence.csv`) contains 15 entries with columns for DOI, citation_key, paper_title, and direct_quote, enabling fact-checking that cited works support the stated claims. For example, the entry for `alphafold2021` includes the quote "achieved median backbone accuracy of 0.96 Å C-alpha RMSD" linked to DOI `10.1038/s41586-021-03819-2`, allowing verification that accuracy claims in the Results section accurately reflect the source.

## Validation and Quality Assurance

Schema-based validation ensures structural consistency across all manuscript outputs through the LinkML schema definition in `schemas/manuscript.yaml` (347 lines). The schema defines six primary classes: `ManuscriptProject` (root container), `ManuscriptOutline` (with required attributes filename pattern `^outline\.md$`, target journal, sections list, word count constraints), `ManuscriptSection` (with filename pattern `^(abstract|introduction|methods|results|discussion|conclusion)\.md$`, section type enumeration, word count minimum 100), `LiteratureResearch` (with three output files: literature.md, literature_citations.bib, literature_evidence.csv), `Critique` (with filename pattern `^critique_(outline|literature|section|manuscript)_v[0-9]+\.md$` and version numbering), and `FullManuscript` (assembled manuscript with required sections validation).

The validation tool `rrwrite-validate-manuscript.py` (453 lines) performs automated checks after each skill execution. Validation operates in four modes specified by the `--type` parameter: `outline` validates manuscript_plan.md structure, `section` validates individual section files against naming conventions and minimum word counts, `literature` validates the trio of literature research outputs, and `manuscript` validates complete assembled manuscripts. For example, validation of a Methods section confirms the filename matches the pattern, the section type is correctly identified, word count exceeds 100, and citation keys follow the bracket notation format `\[[a-z]+[0-9]{4}\]`.

Validation failures produce detailed error reports specifying the violated constraint. A filename mismatch for a Results section stored as `results_draft.md` generates: "ERROR: Invalid filename 'results_draft.md' does not match required pattern '^results\.md$'". A word count violation for a 75-word Abstract generates: "ERROR: Abstract section has 75 words but minimum is 100". Citation integrity checks scan for citation keys (`[author_year]` format) and verify that each key exists in either `references.bib` or `manuscript/literature_citations.bib`, reporting missing citations as validation errors. Figure references are checked for sequential numbering (Figure 1, Figure 2, etc.) with warnings issued for out-of-order references.

Critique iteration tracking provides quality assurance through adversarial review cycles. The `rrwrite-critique-manuscript` skill generates structured reports following the "Reviewer 2" archetype—critical, demanding evidence for claims, focused on reproducibility requirements. Each critique report includes five sections: Summary Assessment (2-3 sentences), Strengths (bulleted list), Major Issues (numbered list with severity HIGH/MEDIUM), Minor Issues (bulleted list), and Compliance Checklist (journal-specific requirements verification). The report concludes with a recommendation (ACCEPT with minor revisions, MAJOR REVISIONS required, or REJECT) and actionable next steps.

Critique reports are versioned using semantic increments: first critique generates `critique_manuscript_v1.md`, revisions generate `critique_manuscript_v2.md`, continuing until quality thresholds are met. The state file tracks critique metrics including number of iterations, recommendation from each iteration, count of major issues (decreasing across iterations indicates improvement), count of minor issues, and timestamps. The `rrwrite-status.py` dashboard displays critique progress: "Critique stage: iteration 2 (v2) - MAJOR REVISIONS required (3 major issues, 7 minor issues)". This quantitative tracking enables assessment of whether revisions adequately address reviewer concerns before submission.

Git history provides auditable provenance for all manuscript changes. Every validation success, section completion, and critique iteration triggers a state update that records the current Git commit hash (obtained via `subprocess.run(['git', 'rev-parse', 'HEAD'])`), branch name, and file modification timestamps. The `rrwrite-compare-runs.py` tool (302 lines) performs line-by-line diffs between archived manuscript versions, highlighting changes in word counts, citation additions/removals, and verification status for numerical claims. This enables answering questions such as "How did the Methods section change between v1 and v2 after addressing Reviewer 2 concerns?" by comparing `.rrwrite/runs/2026-02-01_1430/methods.md` with `manuscript/methods.md`.
