# Discussion

## Advantages Over Manual Writing

RRWrite addresses fundamental limitations of manual manuscript writing by automating the translation of computational research artifacts into publication-ready text while maintaining rigorous fact-checking and provenance tracking. The system reduces transcription errors through mandatory verification of numerical claims against source data files, a critical advantage given that manual transcription of results from CSV files, log outputs, or computational notebooks introduces opportunities for copy-paste mistakes, unit conversion errors, and misalignment between reported statistics and actual data values. The `rrwrite-verify-stats.py` tool enforces verification by computing statistics directly from raw data files and comparing against drafted text, ensuring that reported values (e.g., "mean accuracy of 0.87") match the actual data rather than relying on author memory or manual calculations.

Complete provenance from data files to manuscript claims represents a second major advantage. Traditional writing workflows break provenance chains when authors manually extract numbers from analysis scripts, copy values into manuscripts, and subsequently lose the connection between published claims and their computational origins [kanwal2017]. RRWrite maintains explicit linkages by recording evidence file paths with line number references in the outline (e.g., "Results section claims → data/benchmark_results.csv column 'accuracy' → verification command stored in state.json"), enabling reconstruction of the complete path from raw data through analysis to published text. This provenance tracking aligns with W3C PROV [w3c_prov] and ProvONE [provone] standards for scientific workflow documentation, extending these frameworks to the final publication step.

Rapid journal reformatting enables efficient multi-journal submissions without manual restructuring. The planning skill implements journal-specific templates for Nature Methods (combined Results/Discussion, 800-1200 words), PLOS Computational Biology (separate Results and Discussion, 3000-5000 words total), and Bioinformatics (Application Note format, Algorithm/Implementation sections). Converting a manuscript from Bioinformatics format to PLOS format requires only re-running the planning skill with the target journal specification, which regenerates the outline with appropriate section structure, word count targets, and formatting conventions. This automation reduces the time cost of submitting to multiple journals sequentially (a common practice after rejection) from days of manual reformatting to minutes of automated regeneration.

Iterative revision support through the critique system provides structured feedback analogous to peer review but occurring before submission. The `rrwrite-critique-manuscript` skill implements adversarial review based on journal-specific standards, identifying issues such as missing citations to competing methods, inadequate methodological detail for reproducibility, overclaimed results not supported by evidence, and formatting non-compliance. Unlike human review which occurs after submission delays of weeks to months, automated critique enables rapid iteration during manuscript development. The versioned critique system (v1, v2, v3...) tracks issue resolution across iterations, with quantitative metrics (count of major issues, count of minor issues) indicating convergence toward publication readiness. This addresses concerns raised in [kobak2025excess] about AI-generated text quality by implementing explicit quality gates before finalization.

Reproducibility improvements emerge from the combination of verification enforcement, provenance tracking, and Git integration. Manuscripts generated by RRWrite include explicit links to data files, analysis scripts, and computational notebooks, satisfying reproducibility requirements increasingly mandated by journals and funding agencies [rule2018jupyter]. The schema validation ensures that every quantitative claim references a verifiable source, preventing unsupported statements from entering the final text. Git history provides complete audit trails for manuscript evolution, recording what changed between versions and why (via commit messages), enabling future researchers to understand the development process. This level of documentation transparency exceeds typical manual writing workflows where revision history exists only in author memory or scattered email threads.

## Limitations

RRWrite requires structured project organization following specific conventions for effective operation. The system expects research repositories to contain a `PROJECT.md` file documenting key findings, a `data/` directory with structured data files in CSV or Excel format, a `scripts/` directory with analysis code, and optionally a `figures/` directory with visualization outputs. Projects lacking this organization (e.g., ad-hoc directory structures, data embedded in proprietary formats, analysis performed via point-and-click tools without code artifacts) cannot be processed without preliminary restructuring. This limitation affects researchers in fields where computational workflows are less standardized or where legacy projects predate reproducible research practices.

Python-centric verification tools restrict applicability to research using other computational ecosystems. The `rrwrite-verify-stats.py` tool currently supports only CSV and Excel formats via pandas and openpyxl, excluding common scientific data formats such as HDF5, NetCDF, FITS (astronomy), or domain-specific binary formats. Analysis code verification expects Python scripts with recognizable statistical operations, limiting utility for R-based bioinformatics workflows, MATLAB-based engineering research, or Julia-based numerical computing. Extension to multi-language support would require implementing format readers and verification logic for each ecosystem, substantially increasing system complexity.

English-language focus limits international adoption and multilingual research dissemination. All skills, verification tools, and documentation operate exclusively in English, with citation management supporting only English-language sources effectively. Researchers publishing in non-English journals or writing multilingual manuscripts (e.g., English with German abstracts for European journals) cannot use RRWrite without substantial manual post-processing. The large language model underlying the system (Claude Code) exhibits performance degradation on non-English text [achiam2023], reducing accuracy of fact-checking and citation extraction for multilingual sources. Supporting multiple output languages would require language-specific journal format templates, multilingual citation databases, and validation of translation accuracy for technical terminology.

Dependency on AI model quality introduces variability in output consistency and correctness. While verification tools enforce numerical accuracy, qualitative text generation (e.g., methodology descriptions, result interpretations, discussion framing) inherits the capabilities and limitations of the underlying language model. Recent work demonstrates that AI-generated scientific text exhibits characteristic vocabulary patterns detectable through statistical analysis [kobak2025excess], potentially biasing manuscript acceptance if reviewers identify AI-generated sections. Model limitations in scientific reasoning (e.g., hallucination of plausible-sounding but incorrect technical details, inconsistent application of domain knowledge) require human oversight despite automation benefits. The adversarial critique system partially mitigates this through quality checking, but cannot guarantee elimination of all model-induced errors.

Manual PROJECT.md creation represents a bootstrapping requirement that limits full end-to-end automation. Users must invest time analyzing their repository, summarizing key findings, and documenting evidence file locations before initiating the automated workflow. This upfront cost (typically 30-60 minutes for well-documented projects, longer for complex research with scattered artifacts) creates an adoption barrier, particularly for researchers unfamiliar with structured documentation practices. Future work integrating with computational notebooks could potentially automate PROJECT.md generation by extracting findings from Markdown cells, code comments, and analysis outputs, reducing this manual burden.

## Future Directions

A web dashboard for progress visualization would enhance usability by providing real-time workflow status without command-line queries. The current `rrwrite-status.py` tool displays progress via terminal output, requiring users to explicitly run the command and interpret text-based summaries. A browser-based dashboard could visualize workflow stages graphically (e.g., progress bars for sections completed, charts showing word counts over time, timelines of critique iterations), update automatically as skills execute, and provide clickable links to generated files. Integration with workflow management systems such as WorkflowHub [workflowhub] could enable sharing of manuscript generation workflows as reusable templates, analogous to computational workflow sharing in bioinformatics.

Automatic figure caption generation from plotting scripts would complete the automation pipeline for visual elements. Currently, RRWrite handles text sections but requires manual caption writing for figures. The system could analyze plotting scripts (e.g., matplotlib, ggplot2, Plotly code) to extract axis labels, data sources, visualization types, and statistical annotations, then generate captions following journal conventions (e.g., "Figure 1: Accuracy comparison across models. Boxplots show distribution of test set accuracy for baseline (n=100 runs), improved model (n=100 runs), and state-of-the-art (n=50 runs). Error bars indicate 95% confidence intervals. Asterisks denote statistical significance: * p<0.05, ** p<0.01."). This would require implementing code parsers for multiple plotting libraries and mapping visual elements to standardized caption templates.

Zotero and Mendeley integration would streamline citation management by connecting to researchers' existing reference libraries [rayyan, researchrabbit]. Rather than manually curating `references.bib`, users could grant RRWrite access to their citation manager, enabling automatic citation retrieval, metadata extraction, and bibliography generation. The literature research skill could check for duplicates against the user's library before searching for new sources, avoiding redundant retrieval. Bidirectional synchronization would allow citations discovered during manuscript generation to be added back to the user's permanent library, maintaining consistency across projects.

Export to Overleaf and direct journal submission systems would reduce final formatting steps. Many journals require submission via specific platforms (e.g., Manuscript Central, Editorial Manager) or accept LaTeX source via Overleaf. RRWrite could implement export functions that convert Markdown manuscripts to journal-specific LaTeX templates, package supporting files (figures, data supplements, BibTeX bibliographies), and generate Overleaf-compatible zip archives. Integration with journal APIs could potentially enable one-click submission, analogous to how preprint servers accept direct uploads from computational workflows.

Support for additional journal formats would broaden applicability across scientific disciplines. Current implementation covers three journals popular in computational biology, but researchers in other fields require templates for journals such as Cell, Science, PNAS (biology), NeurIPS, ICML (machine learning), or Biophysical Journal (structural biology). Each journal has unique formatting requirements (section names, word limits, reference styles, figure specifications) necessitating dedicated templates. A community contribution model similar to CSL (Citation Style Language) could enable researchers to contribute templates for their target journals, expanding coverage through distributed effort.

Multi-language support for international research dissemination would extend utility beyond Anglophone research communities. This requires implementing language-specific journal templates (e.g., German formatting for Naturwissenschaften, French for Comptes Rendus), multilingual citation extraction from non-English sources, and verification of translated technical terminology. Leveraging recent advances in multilingual large language models [achiam2023] could enable draft generation in target languages with automated back-translation for quality checking. Partnerships with international research institutions could provide validation datasets for non-English manuscript generation quality assessment.

Automatic table extraction from data files would complement figure caption generation. Scientific manuscripts frequently include tables summarizing experimental results, parameter configurations, or comparison benchmarks. RRWrite could analyze CSV files or dataframes to identify key summary statistics, generate publication-quality tables following journal formatting requirements (e.g., three-line tables for Nature, full-border tables for PLOS), and write captions describing table contents. Table generation logic would need to distinguish between raw data (typically not table-worthy) and aggregated summaries (suitable for publication), potentially using heuristics such as row count thresholds or presence of summary statistics in column names.

Integration with AI-powered literature tools such as Elicit [elicit], SciSpace [scispace], and Paperguide [paperguide] would enhance literature research capabilities beyond basic web search. These platforms provide semantic search over scientific literature, automated paper summarization, citation network analysis, and evidence extraction for specific research questions. RRWrite's literature research skill could query these services via APIs to obtain higher-quality citation recommendations, extract relevant quotes more accurately than web scraping, and identify influential papers through citation graph analysis. This would address limitations of current web search approaches which rely on general search engines not optimized for scientific literature retrieval.
