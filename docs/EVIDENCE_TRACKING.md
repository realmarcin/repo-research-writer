# Evidence Tracking Protocol

**Purpose**: Track all claims in the manuscript with verifiable evidence sources

---

## Overview

RRWrite generates two evidence files to ensure manuscript claims are verifiable:

1. **`literature_evidence.md`** - Evidence from cited papers
2. **`repo_evidence.md`** - Evidence from the repository being documented

These files enable:
- ✅ Fact-checking during critique
- ✅ Reviewer verification during peer review
- ✅ Reproducibility validation
- ✅ Citation accuracy checking

---

## 1. Literature Evidence

### File: `{target_dir}/literature_evidence.md`

**Generated by**: `/rrwrite-research-literature` skill

**Format**:
```markdown
# Literature Evidence

**Generated**: 2026-02-07
**Purpose**: Verbatim quotes supporting cited claims

---

## [citation_key]: [Paper Title]

**Authors**: [Author list]
**Venue**: [Journal/Conference, Year]
**DOI**: [DOI or arXiv]
**URL**: [Direct link]

**Evidence Quote**:
> "[Verbatim quote from abstract or paper]"

**Key Findings**:
- [Specific result, metric, or claim]
- [Novel method or approach]
- [Quantitative improvement if applicable]

---
```

### Example Entry

```markdown
## moxon2025: LinkML: A Linked Data Modeling Language

**Authors**: Moxon S, Solbrig H, Unni D, et al.
**Venue**: GigaScience, 2025
**DOI**: 10.1093/gigascience/giaf152
**URL**: https://doi.org/10.1093/gigascience/giaf152

**Evidence Quote**:
> "LinkML is an open framework that simplifies the process of authoring, validating, and sharing data while reducing heterogeneity, complexity, and the proliferation of single-use data models."

**Key Findings**:
- Open framework for data modeling
- Reduces heterogeneity and complexity
- Supports validation and sharing
- Multi-domain adoption (biology, chemistry, biomedicine)

---
```

### What to Include

**For each cited paper**:
1. Full bibliographic information (authors, venue, year)
2. DOI or arXiv ID (permanent identifier)
3. Direct URL to paper
4. Verbatim quote (1-2 sentences) capturing main contribution
5. Key findings in bullet points

**When to add**:
- During literature research phase
- When drafting sections that cite papers
- When adding new citations during revision

---

## 2. Repository Evidence

### File: `{target_dir}/repo_evidence.md`

**Generated by**: `/rrwrite-draft-section` skill (and manual updates)

**Format**:
```markdown
# Repository Evidence

**Repository**: [GitHub URL]
**Commit**: [hash]
**Analyzed**: [Date]
**Purpose**: Evidence for claims about repository contents

---

## Claim: [Statement from manuscript]

**Section**: [Section name where claim appears]
**Line**: [Approximate line number or paragraph]

**Evidence Source**: [File path, directory, or git data]

**Verification**:
\`\`\`bash
[Command to verify the claim]
\`\`\`

**Output**:
\`\`\`
[Command output proving the claim]
\`\`\`

**Status**: ✅ Verified | ⚠ Approximate | ❌ Unverified

---
```

### Example Entry

```markdown
## Claim: "The repository accumulated 372 commits from 12 contributors"

**Section**: Introduction
**Line**: Paragraph 3

**Evidence Source**: Git repository metadata

**Verification**:
\`\`\`bash
# Count total commits
git rev-list --all --count

# Count unique contributors
git log --all --format='%ae' | sort -u | wc -l
\`\`\`

**Output**:
\`\`\`
372
12
\`\`\`

**Status**: ✅ Verified

---

## Claim: "Package comprises 15 modular files totaling 12,500 lines"

**Section**: Results
**Line**: "Code Organization" subsection

**Evidence Source**: Source code files in src/mypackage/

**Verification**:
\`\`\`bash
# Count source files
find src/mypackage/ -name "*.py" | wc -l

# Count total lines of code
find src/mypackage/ -name "*.py" -exec wc -l {} + | tail -1
\`\`\`

**Output**:
\`\`\`
15
12500 total
\`\`\`

**Status**: ✅ Verified

---

## Claim: "Test suite includes 234 unit tests with 95% coverage"

**Section**: Methods
**Line**: "Testing and Validation" subsection

**Evidence Source**: Test files and coverage report

**Verification**:
\`\`\`bash
# Count tests
pytest --collect-only | grep "test session starts" -A 1

# Check coverage
pytest --cov=src/ --cov-report=term
\`\`\`

**Output**:
\`\`\`
234 tests collected
Coverage: 95%
\`\`\`

**Status**: ✅ Verified

---
```

### What to Include

**For each factual claim**:
1. The exact claim as stated in manuscript
2. Section and approximate location
3. Source file, directory, or data structure
4. Verification command (bash, python, etc.)
5. Command output
6. Verification status (✅ ⚠ ❌)

### Claim Categories

**Code/File Statistics**:
- Line counts
- File counts
- Directory structure
- Language distribution

**Git Metadata**:
- Commit count
- Contributor count
- Commit history
- Branch structure

**Software Outputs**:
- Generated file sizes
- Test results
- Build artifacts
- Documentation

**Data/Content**:
- Dataset sizes
- Record counts
- Field completeness
- Validation results

### When to Add

**During drafting**:
- When making quantitative claims about repository
- When citing specific files or directories
- When describing software outputs

**During critique**:
- When reviewer flags unverified claims
- When fact-checking numerical statements

**Before submission**:
- Verify all claims still accurate at submission commit
- Update commit hash in evidence file

---

## Evidence File Locations

```
manuscript/repo_v1/
├── literature_evidence.md    # Evidence from cited papers
├── repo_evidence.md           # Evidence from repository
├── literature_citations.bib   # BibTeX citations
├── outline.md
├── abstract.md
├── introduction.md
├── ...
```

---

## Verification Workflow

### During Drafting

1. **Write claim** in manuscript section
2. **Add evidence entry** to appropriate file:
   - Literature claim → `literature_evidence.md`
   - Repository claim → `repo_evidence.md`
3. **Run verification command** to confirm accuracy
4. **Mark status** (✅ ⚠ ❌)

### During Critique

1. **Read manuscript** section by section
2. **Identify factual claims** (numbers, statistics, features)
3. **Check evidence file** for supporting entry
4. **Flag missing evidence** as critique issue
5. **Verify claims** by running commands

### Before Submission

1. **Update commit hash** in repo_evidence.md
2. **Re-run all verification commands**
3. **Update outputs** if changed
4. **Mark any stale claims** for revision
5. **Confirm all DOIs** in literature_evidence.md

---

## Automation Support

### Auto-generate Repository Evidence

**Script**: `scripts/rrwrite-extract-repo-evidence.py` (to be created)

```bash
python3 scripts/rrwrite-extract-repo-evidence.py \
  --repo-path /path/to/repo \
  --manuscript manuscript/repo_v1/manuscript.md \
  --output manuscript/repo_v1/repo_evidence.md
```

**What it does**:
1. Scans manuscript for numerical claims
2. Attempts to verify against repository
3. Generates evidence entries with verification commands
4. Flags claims that cannot be auto-verified

### Verify All Evidence

**Script**: `scripts/rrwrite-verify-evidence.py` (to be created)

```bash
python3 scripts/rrwrite-verify-evidence.py \
  --repo-path /path/to/repo \
  --evidence manuscript/repo_v1/repo_evidence.md
```

**What it does**:
1. Parses all verification commands from evidence file
2. Executes each command
3. Compares output to recorded output
4. Reports discrepancies

---

## Integration with Critique

The critique skill should check:

1. **Literature citations**: All cited works have entries in literature_evidence.md
2. **Numerical claims**: All numbers in Results/Methods have repo_evidence.md entries
3. **Verification status**: No ❌ unverified claims in evidence files
4. **Evidence freshness**: Commit hash matches current repository state

**Critique checklist addition**:
```markdown
## Evidence Verification

- [ ] All citations in literature_evidence.md
- [ ] All numerical claims in repo_evidence.md
- [ ] All verification commands run successfully
- [ ] No unverified (❌) claims
- [ ] Commit hash up to date
```

---

## Benefits

### For Authors

✅ **Confidence**: Know every claim is backed by evidence
✅ **Speed**: Quickly find sources when revising
✅ **Accuracy**: Catch errors before submission

### For Reviewers

✅ **Verification**: Easy to check factual claims
✅ **Transparency**: See exact data sources
✅ **Reproducibility**: Can re-run verification commands

### For Readers

✅ **Trust**: Clear evidence trail
✅ **Exploration**: Can explore repository themselves
✅ **Citation**: Easy to find original sources

---

## Status

✅ **Literature evidence format**: Updated to markdown
✅ **Repository evidence protocol**: Documented
⏳ **Auto-extraction script**: To be implemented
⏳ **Verification script**: To be implemented
⏳ **Critique integration**: To be updated

---

## Examples

Complete example manuscripts will be available in the `example/` directory after you generate your first manuscript.

To generate an example manuscript about the RRWrite tool itself:
```bash
/rrwrite --repo /path/to/rrwrite --output-dir example/rrwrite_v1
```

This will create:
- `example/rrwrite_v1/literature_evidence.md` (literature citations)
- `example/rrwrite_v1/repo_evidence.md` (repository evidence)
