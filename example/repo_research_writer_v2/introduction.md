# Introduction

Computational research increasingly relies on code, data, and computational notebooks to generate scientific insights. From protein structure prediction [Jumper2021] to bioinformatics workflows [Nextflow2024], researchers produce comprehensive repositories containing analysis scripts, processed datasets, and visualization code. However, translating these computational artifacts into publication-ready manuscripts remains a predominantly manual process, requiring researchers to carefully extract numerical results, describe methodological details, and maintain accuracy between code outputs and textual claims.

This translation challenge creates several critical issues. First, manual transcription of numerical results from data files to manuscript text is error-prone and time-consuming. Second, maintaining consistency between code implementations and methodological descriptions requires tedious cross-referencing. Third, iterative manuscript revisions necessitate repeated verification that updated claims still match the underlying data. These challenges are exacerbated by journal-specific formatting requirements and word limits that vary substantially across publications [Himmelstein2019].

Existing tools address isolated aspects of this workflow but fail to provide end-to-end automation. Reproducible document systems like Jupyter notebooks [Kluyver2016] and Quarto [USGS2024] enable executable manuscripts that combine code and narrative text, yet they require authors to manually write prose and verify claims. Literature review automation tools [Khalil2024, HasLer2024] assist with citation discovery but do not generate manuscript text from research artifacts. AI-assisted writing systems [Ros2025, CHI2024] improve prose quality but lack integration with computational provenance and fact-checking against source data. Most critically, no existing system connects repository analysis, automated fact verification, and journal-specific manuscript generation into a unified workflow.

The gap between computational research outputs and publication-ready text represents a significant barrier to scientific productivity and reproducibility. Recent advances in fact-checking methodologies [CliVER2024, UIST2024] demonstrate the feasibility of automated claim verification, while the FAIR principles for research software [Barker2022, Wilkinson2016] emphasize the importance of maintaining provenance from data to publication. However, these concepts have not been integrated into practical tools for manuscript generation.

We present RRWrite (Repo Research Writer), a novel system that automates manuscript generation from computational research repositories with integrated fact verification and journal-specific formatting. RRWrite analyzes repository structure to identify data files, analysis scripts, and figures, then generates publication-ready sections where every numerical claim is automatically verified against source data. The system implements four integrated skills for planning, literature research, drafting, and critique, all coordinated through a versioned workflow that supports iterative refinement. RRWrite addresses the complete pipeline from repository analysis to critique-ready manuscripts, maintaining evidence chains from data files to textual claims while adhering to configurable word limits for journals including Bioinformatics, Nature, and PLOS Computational Biology.

The key innovation of RRWrite is the integration of repository analysis with automated fact-checking via statistical verification scripts that validate every numerical assertion against source CSV files. Unlike systems that generate text without verification [Frontiers2025] or workflows that execute code without producing manuscripts [Caprarelli2023, Pimentel2023], RRWrite maintains complete traceability through provenance tracking [JMIR2024] while automating the prose generation process. The system operates externally to research projects, analyzing repositories via URLs or local paths and generating versioned manuscript outputs that can be iteratively refined based on automated critique feedback.
