# Protein Structure Prediction Using Deep Learning

## Project Overview

This project develops a novel deep learning architecture (ProteinNet-Transformer) for predicting protein 3D structures from amino acid sequences. We demonstrate improved accuracy over existing methods (AlphaFold2, RoseTTAFold) on the CASP14 benchmark dataset.

## Manuscript Status

### Target Journal
Bioinformatics (Oxford Academic)

### Current Phase
Planning → Drafting

### Section Status
- [ ] Abstract
- [ ] Introduction
- [ ] Methods
  - [ ] Dataset Description
  - [ ] Model Architecture
  - [ ] Training Procedure
  - [ ] Evaluation Metrics
- [ ] Results
  - [ ] Benchmark Comparison
  - [ ] Ablation Studies
- [ ] Discussion
- [ ] References

## Key Findings

### Main Result
Our ProteinNet-Transformer achieves a mean TM-score of 0.87 on CASP14, representing a 12% improvement over AlphaFold2 baseline (0.78) and 18% over RoseTTAFold (0.74).

**Evidence**:
- Data: `data/benchmark_results.csv` (column: tm_score, row: ProteinNet-Transformer)
- Figure: `figures/accuracy_comparison.png`
- Script: `scripts/evaluate.py` (lines 45-67)

### Secondary Finding
The transformer attention mechanism learns biologically meaningful residue-residue interactions, with 85% of top-weighted attention heads corresponding to known contact maps.

**Evidence**:
- Data: `data/attention_analysis.csv`
- Figure: `figures/attention_heatmap.png`
- Notebook: `notebooks/exploratory_analysis.ipynb` (Cell 12)

### Training Efficiency
Model converges in 48 hours on 4x NVIDIA A100 GPUs, 3x faster than comparable architectures.

**Evidence**:
- Data: `data/training_metrics.csv` (column: convergence_time)
- Figure: `figures/training_curve.png`

## Repository Map

### Data Files

**`data/benchmark_results.csv`**
- Columns: model_name, tm_score, gdt_ts, rmsd, dataset
- Rows: 156 (3 models × 52 protein targets)
- Purpose: Comparison with SOTA methods

**`data/training_metrics.csv`**
- Columns: epoch, loss, validation_loss, convergence_time, gpu_hours
- Rows: 100 (one per training epoch)
- Purpose: Training dynamics and efficiency

**`data/attention_analysis.csv`**
- Columns: protein_id, attention_head, correlation_with_contacts, p_value
- Rows: 480 (10 proteins × 48 attention heads)
- Purpose: Interpretability analysis

### Analysis Scripts

**`scripts/train_model.py`**
- Purpose: Training pipeline for ProteinNet-Transformer
- Key functions:
  - `build_transformer()` (line 34): Model architecture
  - `train_epoch()` (line 102): Training loop
  - `evaluate_metrics()` (line 156): Compute TM-score, GDT-TS
- Dependencies: PyTorch 2.0, transformers 4.30

**`scripts/evaluate.py`**
- Purpose: Benchmark against AlphaFold2 and RoseTTAFold
- Key functions:
  - `load_predictions()` (line 23): Parse prediction files
  - `compute_tm_score()` (line 45): TM-score calculation
  - `generate_comparison_plot()` (line 89): Creates Figure 1
- Outputs: `data/benchmark_results.csv`, `figures/accuracy_comparison.png`

**`scripts/plot_attention.py`**
- Purpose: Visualize transformer attention patterns
- Generates: `figures/attention_heatmap.png`

### Figures

**`figures/accuracy_comparison.png`**
- Type: Bar plot
- Content: TM-scores for ProteinNet-Transformer, AlphaFold2, RoseTTAFold
- Caption: "Comparison of structure prediction accuracy on CASP14 benchmark"
- Generated by: `scripts/evaluate.py:89`

**`figures/training_curve.png`**
- Type: Line plot
- Content: Training and validation loss over 100 epochs
- Caption: "Training dynamics showing convergence at epoch 72"
- Generated by: `scripts/train_model.py:187`

**`figures/attention_heatmap.png`**
- Type: Heatmap
- Content: Attention weights overlaid with true contact map
- Caption: "Transformer attention heads capture residue-residue contacts"
- Generated by: `scripts/plot_attention.py:34`

### Notebooks

**`notebooks/exploratory_analysis.ipynb`**
- Cell 12: Correlation analysis between attention weights and contact maps
- Cell 18: Ablation study removing positional encoding
- Cell 24: Error analysis on failed predictions

## Citation Index

[alphafold2021]: "Highly accurate protein structure prediction with AlphaFold" (Jumper et al., Nature 2021) - DOI: 10.1038/s41586-021-03819-2
[rosettafold2021]: "Accurate prediction of protein structures and interactions using a three-track neural network" (Baek et al., Science 2021) - DOI: 10.1126/science.abj8754
[casp14]: "Critical assessment of techniques for protein structure prediction, 14th round" (Kryshtafovych et al., Proteins 2021) - DOI: 10.1002/prot.26171
[transformer2017]: "Attention is all you need" (Vaswani et al., NeurIPS 2017) - arXiv: 1706.03762
[tmscore2005]: "TM-score: A unified framework for sequence-structure comparison" (Zhang & Skolnick, Proteins 2005) - DOI: 10.1002/prot.20264

**Note**: DOIs enable permanent identification and verification of all cited works. See `literature_evidence.csv` for direct quotes from each paper.

## Methodology Notes

### Model Architecture
- Base: Transformer encoder (12 layers, 768 hidden dim, 12 attention heads)
- Input: Amino acid sequence embeddings + evolutionary features (MSA)
- Output: 3D coordinates for each residue (Cα atoms)
- Novel component: Structure-aware positional encoding

### Training Details
- Dataset: 30,000 proteins from PDB (pre-2018)
- Batch size: 32 proteins
- Optimizer: AdamW (lr=1e-4, weight_decay=0.01)
- Loss: RMSD + TM-score (weighted combination)
- Hardware: 4× NVIDIA A100 (40GB)
- Training time: 48 hours

### Evaluation Protocol
- Benchmark: CASP14 free modeling targets (52 proteins)
- Metrics: TM-score (primary), GDT-TS, RMSD
- Baselines: AlphaFold2 (official predictions), RoseTTAFold (retrained)
- Statistical test: Wilcoxon signed-rank test (p < 0.001)

## Notes for Manuscript

- Emphasis: Novel attention mechanism, not just incremental improvement
- Target audience: Computational biologists and ML practitioners
- Highlight: Open-source code and pre-trained models available
- Limitations: Performance on membrane proteins (future work)
- Data availability: All data deposited to Zenodo (DOI: 10.5281/zenodo.xxxxx)
